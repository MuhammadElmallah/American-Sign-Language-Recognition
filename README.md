# American-Sign-Language-Recognition
This project is trying to have a model that can correctly classify the entire alphabet represented as hand gestures for American Sign Language.

The used model architecture is the MobileNet Architecture as it contains less parameters and then it can be deployed in mobiles and other hardwares easily.


The Notebooks are commented in a very detailed with the tensorboard of the model architecture, the learning curves and so.
